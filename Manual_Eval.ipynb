{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2aadb6-3186-4008-940a-247548c50711",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'dialog_data/dailydialog/'\n",
    "\n",
    "def read_data(data_path):\n",
    "    src = []\n",
    "    trg = []\n",
    "    with open(base_path+data_path) as f:\n",
    "        train_data = f.readlines() #f.read().splitlines()\n",
    "        #print(len(train_data))\n",
    "        for line in train_data:\n",
    "            utterences = line.split('__eou__')\n",
    "            del utterences[-1] #last item is '\\n'\n",
    "            src_utterence = []\n",
    "            for idx in range(len(utterences)-1):\n",
    "                # src_utterence=src_utterence+'</s>'+trg_utterence\n",
    "                src_utterence.append(utterences[idx])\n",
    "                \n",
    "                if(len(src_utterence)>5):\n",
    "                   src_utterence = src_utterence[-5:]\n",
    "                \n",
    "                src.append('</s>'.join(src_utterence))\n",
    "                trg_utterence = utterences[idx+1].strip()\n",
    "                #trg_utterence = trg_polite_dict[utterences[idx+1]]\n",
    "                trg.append(trg_utterence)\n",
    "                \n",
    "        return src,trg\n",
    "train_src, train_trg = read_data('train/dialogues_train.txt')\n",
    "print(len(train_src), len(train_trg))\n",
    "dev_src, dev_trg = read_data('validation/dialogues_validation.txt')\n",
    "print(len(dev_src), len(dev_trg))\n",
    "test_src, test_trg = read_data('test/dialogues_test.txt')\n",
    "print(len(test_src), len(test_trg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8262ca7e-41ed-4483-a79b-d1c65d84c8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bc7070-a851-4d94-989f-4d71bc271738",
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla = list()\n",
    "with open('dialog/daily_dialog/without-finetune/dlg_responses_wo_pred_lists.pkl', 'rb') as f:\n",
    "    vanilla = pickle.load(f)\n",
    "print(len(vanilla))\n",
    "\n",
    "finetune = list()\n",
    "with open('dialog/daily_dialog/finetune/dlg_responses_direct_pred_lists.pkl', 'rb') as f:\n",
    "    finetune = pickle.load(f)\n",
    "print(len(finetune))\n",
    "\n",
    "finetune_taggen = list()\n",
    "with open('dialog/daily_dialog/finetune-polite/Blenderbot_polite_pred_tag-gen.pkl', 'rb') as f:\n",
    "    finetune_taggen = pickle.load(f)\n",
    "print(len(finetune_taggen))\n",
    "\n",
    "finetune_ours = list()\n",
    "with open('dialog/daily_dialog/finetune-polite/polite_dlg_responses_direct_pred_lists.pkl', 'rb') as f:\n",
    "    finetune_ours = pickle.load(f)\n",
    "print(len(finetune_ours))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a1dedd-7f4d-4ee8-a9d6-7eeb227c9595",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(97,99):\n",
    "    print('Context')\n",
    "    print(10*'-')\n",
    "    print(test_src[idx])\n",
    "    \n",
    "    print('\\nUtterance')\n",
    "    print(10*'-')\n",
    "    print(test_trg[idx])\n",
    "    \n",
    "    print('\\nVanila')\n",
    "    print(10*'-')\n",
    "    print(vanilla[idx])\n",
    "    \n",
    "    print('\\nFinetune')\n",
    "    print(10*'-')\n",
    "    print(finetune[idx])\n",
    "    \n",
    "    print('\\nFinetune Tag-gen')\n",
    "    print(10*'-')\n",
    "    print(finetune_taggen[idx].replace('averaging',''))\n",
    "    \n",
    "    print('\\nFinetune Ours')\n",
    "    print(10*'-')\n",
    "    print(finetune_ours[idx])\n",
    "    \n",
    "    print('\\n')\n",
    "    print(idx)\n",
    "    \n",
    "    print(50*'=')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df189823-0209-4176-8a0b-a2598f1f7c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "custom_list = random.sample(range(0, 1000), 100)\n",
    "print(custom_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28c0886-853f-4af4-8d7e-dd9032c0f7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7460380b-dfaa-4dd6-bea1-e4c1d89c8f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = list()\n",
    "for idx in custom_list:\n",
    "    dict1 = {'id': 'vanilla', 'context': test_src[idx], 'utterance': test_trg[idx], 'pred_utterance': vanilla[idx]}\n",
    "    dict2 = {'id': 'finetune', 'context': test_src[idx], 'utterance': test_trg[idx], 'pred_utterance': finetune[idx]}\n",
    "    dict3 = {'id': 'finetune_taggen', 'context': test_src[idx], 'utterance': test_trg[idx], 'pred_utterance': finetune_taggen[idx]}\n",
    "    dict4 = {'id': 'finetune_ours', 'context': test_src[idx], 'utterance': test_trg[idx], 'pred_utterance': finetune_ours[idx]}\n",
    "    dict_list = [dict1,dict2,dict3,dict4] \n",
    "    random.shuffle(dict_list)\n",
    "    for d in dict_list:\n",
    "        eval_data.append(d)\n",
    "df = pd.DataFrame(eval_data)\n",
    "df.to_csv('human_evals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b0fb3f-71e9-4087-ac34-e85c628b3479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "SHEET_ID = '1sS48z1jTin_eyKbHF5oCwY2-xA-cgeEd6cLlhPlTJUA'\n",
    "SHEET_NAME = 'Polite_Chatbot_Human_Evals'\n",
    "url = f'https://docs.google.com/spreadsheets/d/{SHEET_ID}/gviz/tq?tqx=out:csv&sheet={SHEET_NAME}'\n",
    "df = pd.read_csv(url)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4973a-f058-430b-ac90-6bd887672beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['id'].values.tolist()[199], df['politeness'].values.tolist()[199], df['coherent-to-context'].values.tolist()[199], df['grammatical-quality'].values.tolist()[199])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2197acc7-0918-46f0-bfc5-a149226f942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_score(id_list, metric_list):\n",
    "    vanila = list()\n",
    "    finetune = list()\n",
    "    finetune_taggen = list()\n",
    "    finetune_ours = list()\n",
    "    for idx, i in enumerate(id_list):\n",
    "        if i == 'vanilla':\n",
    "            vanila.append(int(metric_list[idx]))\n",
    "        elif i == 'finetune':\n",
    "            finetune.append(int(metric_list[idx]))\n",
    "        elif i == 'finetune_taggen':\n",
    "            finetune_taggen.append(int(metric_list[idx]))\n",
    "        elif i == 'finetune_ours':\n",
    "            finetune_ours.append(int(metric_list[idx]))\n",
    "    return sum(vanila)/len(vanila),sum(finetune)/len(finetune),sum(finetune_taggen)/len(finetune_taggen),sum(finetune_ours)/len(finetune_ours) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197366e3-147f-46c5-995a-2b2f084cca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = avg_score(df['id'].values.tolist()[0:200], df['politeness'].values.tolist()[0:200])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c22d8e-f2cd-455c-8a10-e55cf6992597",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = avg_score(df['id'].values.tolist()[0:200], df['coherent-to-context'].values.tolist()[0:200])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15dc7fe-e8b8-41fa-9b30-fc471fa36bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = avg_score(df['id'].values.tolist()[0:200], df['grammatical-quality'].values.tolist()[0:200])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047edaa0-868b-4cbb-8af8-d6ad043210bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
