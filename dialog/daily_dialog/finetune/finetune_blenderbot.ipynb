{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848ce54d-115a-4334-a2d1-95770acf49de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets\n",
    "# !jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fca4405-36ff-4804-8d03-4b19a1874fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ipywidgets import FloatProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3293881c-975d-4081-b712-318e99c178d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b825788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../../../dialog_data/dailydialog/'\n",
    "\n",
    "def read_data(data_path):\n",
    "    src = []\n",
    "    trg = []\n",
    "    with open(base_path+data_path) as f:\n",
    "        train_data = f.readlines() #f.read().splitlines()\n",
    "        #print(len(train_data))\n",
    "        for line in train_data:\n",
    "            utterences = line.split('__eou__')\n",
    "            del utterences[-1] #last item is '\\n'\n",
    "            src_utterence = []\n",
    "            for idx in range(len(utterences)-1):\n",
    "                # src_utterence=src_utterence+'</s>'+trg_utterence\n",
    "                src_utterence.append(utterences[idx])\n",
    "                \n",
    "                if(len(src_utterence)>5):\n",
    "                   src_utterence = src_utterence[-5:]\n",
    "                \n",
    "                src.append('</s>'.join(src_utterence))\n",
    "                trg_utterence = utterences[idx+1]\n",
    "                #trg_utterence = trg_polite_dict[utterences[idx+1]]\n",
    "                trg.append(trg_utterence)\n",
    "                \n",
    "        return src,trg\n",
    "train_src, train_trg = read_data('train/dialogues_train.txt')\n",
    "print(len(train_src), len(train_trg))\n",
    "dev_src, dev_trg = read_data('validation/dialogues_validation.txt')\n",
    "print(len(dev_src), len(dev_trg))\n",
    "test_src, test_trg = read_data('test/dialogues_test.txt')\n",
    "print(len(test_src), len(test_trg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997731cf-07ba-4f28-8c6a-b55340c66faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_src = train_src[0:2]\n",
    "# train_trg = train_trg[0:2]\n",
    "# dev_src = dev_src[0:2]\n",
    "# dev_trg = dev_trg[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8521462-5a43-40c3-b2ed-26b5b5759119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# res_path = 'daily_dialog/responses/'\n",
    "# with open(res_path+'train_res.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_trg, f)\n",
    "# with open(res_path+'dev_res.pkl', 'wb') as f:\n",
    "#     pickle.dump(dev_trg, f)\n",
    "# with open(res_path+'test_res.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_trg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceed2fb4-0218-4208-84d8-2053a341c65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_path + 'train/dialogues_train.txt') as myfile:\n",
    "    head = [next(myfile) for x in range(1)]\n",
    "print(head)\n",
    "\n",
    "length = len(head[0].split('__eou__'))\n",
    "\n",
    "print('\\n')\n",
    "for idx in range(length):\n",
    "    print(train_src[idx])\n",
    "    print(train_trg[idx])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d708e149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.DataFrame(\n",
    "    {'src': train_src,\n",
    "     'trg': train_trg\n",
    "    })\n",
    "train_df.head()\n",
    "\n",
    "dev_df = pd.DataFrame(\n",
    "    {'src': dev_src,\n",
    "     'trg': dev_trg\n",
    "    })\n",
    "dev_df.head()\n",
    "\n",
    "test_df = pd.DataFrame(\n",
    "    {'src': test_src,\n",
    "     'trg': test_trg\n",
    "    })\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6593855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"90MBB_facebook/blenderbot_small-90M/checkpoint-87000/\"\n",
    "# model_name = \"facebook/blenderbot_small-90M\"\n",
    "# model_name = 'facebook/blenderbot-400M-distill/checkpoint-87176'\n",
    "model_name = 'facebook/blenderbot-400M-distill/checkpoint-19014'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f10e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "# from transformers import BlenderbotSmallTokenizer\n",
    "from transformers import BlenderbotTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = BlenderbotTokenizer.from_pretrained(model_name)\n",
    "#tokenizer = BlenderbotSmallTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_src_encodings = tokenizer(train_df['src'].values.tolist(), truncation=True, padding=True, max_length=128)\n",
    "train_trg_encodings = tokenizer(train_df['trg'].values.tolist(), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "dev_src_encodings = tokenizer(dev_df['src'].values.tolist(), truncation=True, padding=True, max_length=128)\n",
    "dev_trg_encodings = tokenizer(dev_df['trg'].values.tolist(), truncation=True, padding=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ce531c-d740-4a6c-aa06-8432304d2c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in train_src_encodings.items():\n",
    "    print(key, len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbead599",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        \n",
    "        item['labels'] = torch.tensor(self.labels['input_ids'][idx])\n",
    "        # item['decoder_input_ids'] = torch.tensor(self.labels['input_ids'][idx])\n",
    "        # item['decoder_attention_mask'] = torch.tensor(self.labels['attention_mask'][idx])\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f712cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CreateDataset(train_src_encodings, train_trg_encodings)\n",
    "dev_dataset = CreateDataset(dev_src_encodings, dev_trg_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5534fde9-de63-4d52-ac78-4018722b1adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795e839d-1e3b-4ee7-9274-ec7fb1f5c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00fb2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "# metric = datasets.load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e8f290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "#     # Replace -100 in the labels as we can't decode them.\n",
    "#     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "#     # Rouge expects a newline after each sentence\n",
    "#     decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "#     decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "#     result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "#     # Extract a few results\n",
    "#     result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "#     # Add mean generated length\n",
    "#     prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "#     result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "#     return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ae6097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlenderbotForConditionalGeneration\n",
    "# from transformers import BlenderbotSmallForConditionalGeneration\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "model = BlenderbotForConditionalGeneration.from_pretrained(model_name)\n",
    "#model = BlenderbotSmallForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "batch_size = 8\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    model_name,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    num_train_epochs=8,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffdeac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d6eb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    "    #compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41788bbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8c2c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dc92c3-261d-43ed-8aa6-91c8be9ddd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(src):\n",
    "    src_tknz = tokenizer(src, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "    generated_ids = model.generate(src_tknz[\"input_ids\"].cuda(), max_length=128)\n",
    "\n",
    "    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5080dd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(50):\n",
    "    print('src:', test_df['src'].values.tolist()[idx])\n",
    "    print('trg:', test_df['trg'].values.tolist()[idx])\n",
    "\n",
    "    print('pred:', gen(test_df['src'].values.tolist()[idx]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e24411b-7f86-42d3-b79d-02f2c907223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fdb789-9911-421d-b6f8-0614ce63c054",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlg_responses_pred_direct = []\n",
    "for idx in range(len(test_df['src'].values.tolist())):\n",
    "    dlg_responses_pred_direct.append((gen(test_df['src'].values.tolist()[idx])))\n",
    "\n",
    "print(len(dlg_responses_pred_direct))\n",
    "\n",
    "with open('dlg_responses_direct_pred_lists.pkl', 'wb') as f:\n",
    "    pickle.dump(dlg_responses_pred_direct, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e805f7e-166c-4667-9fb9-ffeac460520e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
